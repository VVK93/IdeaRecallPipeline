2025-04-01 14:45:01,797 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:04,912 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:04,916 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:04,916 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:04,916 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:04,917 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:11,266 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:14,442 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:14,449 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:14,451 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:14,454 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:14,455 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:50:02,805 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:05,867 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:05,871 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:05,871 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:05,872 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:12,241 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:15,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:15,159 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:15,159 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:15,161 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:34,224 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:34,234 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:34,284 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501034.2843058 ---
2025-04-01 14:50:34,285 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:50:52,035 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video explains the AI evaluation pipeline, which consists of four stages. Stage one employs automated checks such as BERTScore and length constraints. Stage two utilizes an AI Judg...
2025-04-01 14:50:52,038 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:50:52,039 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:50:52,039 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:50:52,039 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:50:52,040 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:50:52,041 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:50:55,418 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:50:55,419 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:50:55,419 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:50:55,420 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:50:56,645 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:50:56,645 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:50:56,645 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:50:56,646 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:50:56,646 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501034.2843058. New log length: 1
2025-04-01 14:50:56,646 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501034.2843058, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.7128249406814575, 'message': 'BERTScore F1: 0.7128', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:51:36,438 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:39,970 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:39,976 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:39,976 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:39,979 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:48,684 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:51,442 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:51,444 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:51,445 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:51,447 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:53,018 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:53,019 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:53,027 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501113.027596 ---
2025-04-01 14:51:53,029 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:52:02,237 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:52:02,238 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:52:02,238 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:52:02,239 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:52:02,239 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:52:02,239 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:52:02,240 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:52:05,445 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:52:05,445 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:52:05,446 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:52:05,446 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:52:06,567 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:52:06,567 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:52:06,568 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:52:06,569 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:52:06,569 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501113.027596. New log length: 1
2025-04-01 14:52:06,570 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501113.027596, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.713367760181427, 'message': 'BERTScore F1: 0.7134', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:52:06,752 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:52:06,753 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:52:06,756 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,777 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 14:52:06,778 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,779 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:28,873 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:32,996 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:33,002 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 15:21:33,002 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 15:21:33,006 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:44,751 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:44,752 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:44,803 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743502904.8032691 ---
2025-04-01 15:21:44,805 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 15:22:09,153 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. In stage one, automated checks such as BERTScore and length constraints are employed. Stage two involves an AI Judge, specifica...
2025-04-01 15:22:09,156 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 15:22:09,156 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 15:22:09,156 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 15:22:09,157 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 15:22:09,157 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:22:09,170 - INFO - app.py:318 - Length check result: Passed=False, Details={'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:22:12,582 - INFO - app.py:329 - BERTScore calculated: Score=0.693, PassedThreshold=True
2025-04-01 15:22:12,583 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:22:12,583 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:22:12,584 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:22:13,856 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:22:13,857 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 15:22:13,857 - INFO - app.py:356 - Successfully parsed AI Judge response.
2025-04-01 15:22:13,857 - DEBUG - app.py:357 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:22:13,858 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:22:13,858 - INFO - app.py:379 - Evaluation results stored for timestamp 1743502904.8032691. New log length: 1
2025-04-01 15:22:13,858 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743502904.8032691, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6926636099815369, 'message': 'BERTScore F1: 0.6927', 'passed_threshold': True}, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 15:22:14,117 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:14,118 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:14,123 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,174 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:14,175 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,177 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:43,863 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:43,865 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:43,920 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,949 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:43,955 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,962 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:44,530 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,530 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,545 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,550 - INFO - app.py:494 - User submitted rating: 3 for log index 0 (Timestamp: 1743502904.8032691)
2025-04-01 15:22:44,671 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,671 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,675 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,679 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:44,680 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,682 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:06,223 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:09,502 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:09,507 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:09,507 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:09,509 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:19,174 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:22,191 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:22,195 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:22,195 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:22,197 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:23,794 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:23,795 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:23,797 - INFO - app.py:254 - --- Pipeline Run Button Clicked: Timestamp 1743504383.7977111 ---
2025-04-01 15:46:23,798 - INFO - app.py:265 - Initiating Generation Stage.
2025-04-01 15:46:43,111 - DEBUG - app.py:272 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage employs an AI Judge, speci...
2025-04-01 15:46:43,120 - INFO - app.py:289 - Checking format of generated output.
2025-04-01 15:46:43,121 - INFO - app.py:294 - Generated output format check passed.
2025-04-01 15:46:43,121 - DEBUG - app.py:298 - Successfully parsed generated data.
2025-04-01 15:46:43,121 - INFO - app.py:310 - Starting Evaluation Stages.
2025-04-01 15:46:43,122 - INFO - app.py:313 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:46:43,129 - INFO - app.py:321 - Length check result: Passed=False, Details={'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:46:46,487 - INFO - app.py:332 - BERTScore calculated: Score=0.671, PassedThreshold=True
2025-04-01 15:46:46,487 - INFO - app.py:340 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:46:46,487 - DEBUG - app.py:344 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:46:46,488 - INFO - app.py:349 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:46:47,813 - DEBUG - app.py:352 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:46:47,814 - INFO - app.py:358 - AI Judge call successful, parsing response.
2025-04-01 15:46:47,814 - INFO - app.py:362 - Successfully parsed AI Judge response.
2025-04-01 15:46:47,814 - DEBUG - app.py:363 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:46:47,816 - INFO - app.py:380 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:46:47,816 - INFO - app.py:390 - Evaluation results stored for timestamp 1743504383.7977111. New log length: 1
2025-04-01 15:46:47,817 - DEBUG - app.py:391 - Full evaluation results for current run: {'timestamp': 1743504383.7977111, 'generation_time': 19.322030782699585, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6705673336982727, 'message': 'BERTScore F1: 0.6706', 'passed_threshold': True}, 'stage1_time': 3.3644537925720215, 'stage2_time': 1.3285832405090332, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 3.0994415283203125e-06}
2025-04-01 15:46:47,999 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:48,000 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:48,003 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,028 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:46:48,029 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,031 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:09,721 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:09,722 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:09,737 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,752 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:09,755 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,761 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:11,041 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,041 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,051 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,055 - INFO - app.py:505 - User submitted rating: 3 for log index 0 (Timestamp: 1743504383.7977111)
2025-04-01 15:47:11,165 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,165 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,173 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,180 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:11,181 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,187 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:34,051 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:37,162 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:37,166 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:37,166 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:37,167 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:46,128 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:49,486 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:49,489 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:49,490 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:49,492 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:52,155 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:52,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:52,186 - INFO - app.py:263 - --- Pipeline Run Button Clicked: Timestamp 1743504712.186294 ---
2025-04-01 15:51:52,186 - INFO - app.py:274 - Initiating Generation Stage.
2025-04-01 15:52:08,718 - DEBUG - app.py:281 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. In the second stage, an AI Judge, specifica...
2025-04-01 15:52:08,719 - INFO - app.py:298 - Checking format of generated output.
2025-04-01 15:52:08,719 - INFO - app.py:303 - Generated output format check passed.
2025-04-01 15:52:08,719 - DEBUG - app.py:307 - Successfully parsed generated data.
2025-04-01 15:52:08,720 - INFO - app.py:319 - Starting Evaluation Stages.
2025-04-01 15:52:08,720 - INFO - app.py:322 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:52:08,725 - INFO - app.py:330 - Length check result: Passed=False, Details={'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:52:12,363 - INFO - app.py:341 - BERTScore calculated: Score=0.680, PassedThreshold=True
2025-04-01 15:52:12,364 - INFO - app.py:349 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:52:12,364 - DEBUG - app.py:353 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:52:12,365 - INFO - app.py:358 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:52:13,523 - DEBUG - app.py:361 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:52:13,524 - INFO - app.py:367 - AI Judge call successful, parsing response.
2025-04-01 15:52:13,525 - INFO - app.py:371 - Successfully parsed AI Judge response.
2025-04-01 15:52:13,525 - DEBUG - app.py:372 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:52:13,525 - INFO - app.py:389 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:52:13,526 - INFO - app.py:399 - Evaluation results stored for timestamp 1743504712.186294. New log length: 1
2025-04-01 15:52:13,526 - DEBUG - app.py:400 - Full evaluation results for current run: {'timestamp': 1743504712.186294, 'generation_time': 16.53201913833618, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6795765161514282, 'message': 'BERTScore F1: 0.6796', 'passed_threshold': True}, 'stage1_time': 3.644177198410034, 'stage2_time': 1.16099214553833, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 2.1457672119140625e-06}
2025-04-01 15:52:13,708 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:13,708 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:13,712 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,738 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:13,739 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,741 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,239 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,240 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,260 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,272 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:31,273 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,276 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,933 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,934 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,944 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,956 - INFO - app.py:514 - User submitted rating: 2 for log index 0 (Timestamp: 1743504712.186294)
2025-04-01 15:52:32,062 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:32,062 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:32,066 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,070 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:32,071 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,074 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:02,379 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:05,324 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:57:05,330 - DEBUG - app.py:312 - Initialized evaluation_log in session state.
2025-04-01 15:57:05,330 - DEBUG - app.py:316 - Initialized current_run_index in session state.
2025-04-01 15:57:05,331 - INFO - app.py:662 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:12,526 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:16,436 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:57:16,450 - DEBUG - app.py:312 - Initialized evaluation_log in session state.
2025-04-01 15:57:16,451 - DEBUG - app.py:316 - Initialized current_run_index in session state.
2025-04-01 15:57:16,474 - INFO - app.py:662 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:22,586 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:22,588 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:03,038 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:06,566 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:06,576 - DEBUG - app.py:277 - Initialized evaluation_log in session state.
2025-04-01 16:03:06,576 - DEBUG - app.py:281 - Initialized current_run_index in session state.
2025-04-01 16:03:06,578 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:15,086 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:18,159 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:18,177 - DEBUG - app.py:277 - Initialized evaluation_log in session state.
2025-04-01 16:03:18,178 - DEBUG - app.py:281 - Initialized current_run_index in session state.
2025-04-01 16:03:18,197 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:33,243 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:33,245 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:33,272 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:35,334 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:35,335 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:31,951 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:35,086 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:35,092 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:05:35,092 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:05:35,096 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:40,978 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:43,984 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:43,988 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:05:43,989 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:05:43,991 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:55,993 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:55,994 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:56,030 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:57,182 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:57,182 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:17,883 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:20,892 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:20,898 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:07:20,898 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:07:20,900 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:28,883 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:31,941 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:31,944 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:07:31,945 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:07:31,947 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:35,979 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:35,981 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:35,990 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:36,865 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:36,865 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:45,593 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:41,732 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:44,770 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:44,775 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:08:44,775 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:08:44,777 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:52,713 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:55,833 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:55,837 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:08:55,837 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:08:55,839 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:58,979 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:58,980 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:59,030 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:09:00,548 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:09:00,548 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:09:04,234 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743505744.234179 ---
2025-04-01 16:09:04,235 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:09:16,168 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video provides a series of guided stretching exercises focusing on various parts of the body, including the hips, spine, shoulders, and legs. Each stretch is held for a specific du...
2025-04-01 16:09:16,170 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:09:16,171 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:09:16,171 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:09:16,171 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:09:16,172 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:09:16,182 - INFO - app.py:351 - Length check result: Passed=True, Details={'summary_tokens': 138, 'flashcards_tokens': 128, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 16:09:20,218 - INFO - app.py:362 - BERTScore calculated: Score=-0.455, PassedThreshold=False
2025-04-01 16:09:20,219 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:09:20,219 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:09:20,219 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:09:22,301 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:09:22,301 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:09:22,302 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:09:22,302 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:09:22,302 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:09:22,302 - INFO - app.py:420 - Evaluation results stored for timestamp 1743505744.234179. New log length: 1
2025-04-01 16:09:22,302 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743505744.234179, 'generation_time': 11.935593128204346, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 138, 'flashcards_tokens': 128, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': -0.45474475622177124, 'message': 'BERTScore F1: -0.4547', 'passed_threshold': False}, 'stage1_time': 4.046546936035156, 'stage2_time': 2.08321475982666, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 0.0}
2025-04-01 16:09:22,470 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:09:22,470 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:09:22,473 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505744.234179
2025-04-01 16:09:22,501 - DEBUG - app.py:597 - Displaying details for Log ID 1
2025-04-01 16:09:22,502 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505744.234179
2025-04-01 16:09:22,504 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:06,160 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:09,514 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:09,520 - DEBUG - app.py:275 - Initialized evaluation_log in session state.
2025-04-01 16:11:09,520 - DEBUG - app.py:279 - Initialized current_run_index in session state.
2025-04-01 16:11:09,521 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:20,121 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:24,054 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:24,058 - DEBUG - app.py:275 - Initialized evaluation_log in session state.
2025-04-01 16:11:24,059 - DEBUG - app.py:279 - Initialized current_run_index in session state.
2025-04-01 16:11:24,063 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:28,902 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:28,905 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:28,969 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:30,429 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:30,430 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:34,119 - INFO - app.py:284 - --- Pipeline Run Initiated: Timestamp 1743505894.119435 ---
2025-04-01 16:11:34,120 - INFO - app.py:298 - Initiating Generation Stage.
2025-04-01 16:11:46,283 - DEBUG - app.py:305 - Generator Raw Response Snippet: {
  "summary": "The discussion focuses on the anterior midcingulate cortex, a brain area that enlarges when individuals engage in activities they dislike, such as exercise or dieting. This enlargement...
2025-04-01 16:11:46,284 - INFO - app.py:322 - Checking format of generated output.
2025-04-01 16:11:46,285 - INFO - app.py:327 - Generated output format check passed.
2025-04-01 16:11:46,285 - DEBUG - app.py:331 - Successfully parsed generated data.
2025-04-01 16:11:46,285 - INFO - app.py:343 - Starting Evaluation Stages.
2025-04-01 16:11:46,285 - INFO - app.py:346 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:11:46,300 - INFO - app.py:354 - Length check result: Passed=False, Details={'summary_tokens': 140, 'flashcards_tokens': 156, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 16:11:50,415 - INFO - app.py:365 - BERTScore calculated: Score=-0.353, PassedThreshold=False
2025-04-01 16:11:50,416 - INFO - app.py:373 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:11:50,416 - DEBUG - app.py:377 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:11:50,416 - INFO - app.py:382 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:11:51,950 - DEBUG - app.py:385 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:11:51,950 - INFO - app.py:391 - AI Judge call successful, parsing response.
2025-04-01 16:11:51,951 - INFO - app.py:395 - Successfully parsed AI Judge response.
2025-04-01 16:11:51,951 - DEBUG - app.py:396 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:11:51,952 - INFO - app.py:413 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:11:51,953 - INFO - app.py:423 - Evaluation results stored for timestamp 1743505894.119435. New log length: 1
2025-04-01 16:11:51,953 - DEBUG - app.py:424 - Full evaluation results for current run: {'timestamp': 1743505894.119435, 'generation_time': 12.163750171661377, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 140, 'flashcards_tokens': 156, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': -0.3531835079193115, 'message': 'BERTScore F1: -0.3532', 'passed_threshold': False}, 'stage1_time': 4.130232095718384, 'stage2_time': 1.5359771251678467, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 2.1457672119140625e-06}
2025-04-01 16:11:52,162 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:52,163 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:52,166 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505894.119435
2025-04-01 16:11:52,196 - DEBUG - app.py:600 - Displaying details for Log ID 1
2025-04-01 16:11:52,196 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505894.119435
2025-04-01 16:11:52,198 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:14:23,388 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:14:26,533 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:14:26,539 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:14:26,539 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:14:26,544 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:14:34,239 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:14:37,082 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:14:37,086 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:14:37,087 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:14:37,089 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:15:04,344 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:04,347 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:04,370 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:15:05,729 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:05,729 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:11,183 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743506111.1835551 ---
2025-04-01 16:15:11,184 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:15:24,199 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video transcript describes a series of stretching exercises focusing on different body parts including the hamstrings, quads, hips, and groin. Key stretches demonstrated include dy...
2025-04-01 16:15:24,200 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:15:24,200 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:15:24,201 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:15:24,201 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:15:24,201 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:15:24,208 - INFO - app.py:351 - Length check result: Passed=True, Details={'summary_tokens': 115, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 16:15:28,480 - INFO - app.py:362 - BERTScore calculated: Score=-0.477, PassedThreshold=False
2025-04-01 16:15:28,482 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:15:28,482 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:15:28,482 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:15:30,404 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:15:30,404 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:15:30,404 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:15:30,404 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:15:30,405 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:15:30,405 - INFO - app.py:420 - Evaluation results stored for timestamp 1743506111.1835551. New log length: 1
2025-04-01 16:15:30,405 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743506111.1835551, 'generation_time': 13.015744924545288, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 115, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': -0.4767054319381714, 'message': 'BERTScore F1: -0.4767', 'passed_threshold': False}, 'stage1_time': 4.280241250991821, 'stage2_time': 1.922940731048584, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 9.5367431640625e-07}
2025-04-01 16:15:30,581 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:30,582 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:30,585 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506111.1835551
2025-04-01 16:15:30,614 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-01 16:15:30,615 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506111.1835551
2025-04-01 16:15:30,617 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:03,835 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:06,959 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:06,965 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:17:06,965 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:17:06,967 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:14,619 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:17,779 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:17,785 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:17:17,785 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:17:17,788 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:20,696 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:20,697 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:20,732 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:22,267 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:22,268 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:22,289 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:32,122 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:32,125 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:32,165 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:32,347 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:32,347 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:33,445 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:18:48,764 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:18:51,776 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:18:51,781 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:18:51,781 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:18:51,784 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:18:58,433 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:01,818 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:01,829 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:19:01,830 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:19:01,832 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:19:15,544 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:15,547 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:15,572 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:19:17,112 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:17,112 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:18,169 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:38,309 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:41,416 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:41,428 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:21:41,428 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:21:41,430 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:47,570 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:50,424 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:50,429 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:21:50,429 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:21:50,431 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:53,762 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:53,763 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:53,772 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:54,825 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:54,828 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:55,850 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743506515.850836 ---
2025-04-01 16:21:55,851 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:22:21,704 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video demonstrates a series of dynamic and static stretching exercises targeting various muscle groups. It starts with a dynamic lunge to hamstring stretch, transitioning between b...
2025-04-01 16:22:21,707 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:22:21,707 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:22:21,707 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:22:21,708 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:22:21,708 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:22:21,719 - INFO - app.py:351 - Length check result: Passed=False, Details={'summary_tokens': 170, 'flashcards_tokens': 205, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 16:22:25,650 - INFO - app.py:362 - BERTScore calculated: Score=-0.175, PassedThreshold=False
2025-04-01 16:22:25,652 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:22:25,652 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:22:25,652 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:22:26,957 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:22:26,957 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:22:26,958 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:22:26,958 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:22:26,959 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:22:26,960 - INFO - app.py:420 - Evaluation results stored for timestamp 1743506515.850836. New log length: 1
2025-04-01 16:22:26,960 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743506515.850836, 'generation_time': 25.85506820678711, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 170, 'flashcards_tokens': 205, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': -0.17490310966968536, 'message': 'BERTScore F1: -0.1749', 'passed_threshold': False}, 'stage1_time': 3.9433281421661377, 'stage2_time': 1.3073318004608154, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 1.9073486328125e-06}
2025-04-01 16:22:27,138 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:22:27,139 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:22:27,142 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506515.850836
2025-04-01 16:22:27,169 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-01 16:22:27,169 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506515.850836
2025-04-01 16:22:27,171 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
