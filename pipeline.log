2025-04-01 14:45:01,797 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:04,912 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:04,916 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:04,916 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:04,916 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:04,917 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:11,266 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:14,442 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:14,449 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:14,451 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:14,454 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:14,455 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:50:02,805 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:05,867 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:05,871 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:05,871 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:05,872 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:12,241 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:15,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:15,159 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:15,159 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:15,161 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:34,224 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:34,234 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:34,284 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501034.2843058 ---
2025-04-01 14:50:34,285 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:50:52,035 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video explains the AI evaluation pipeline, which consists of four stages. Stage one employs automated checks such as BERTScore and length constraints. Stage two utilizes an AI Judg...
2025-04-01 14:50:52,038 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:50:52,039 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:50:52,039 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:50:52,039 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:50:52,040 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:50:52,041 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:50:55,418 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:50:55,419 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:50:55,419 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:50:55,420 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:50:56,645 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:50:56,645 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:50:56,645 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:50:56,646 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:50:56,646 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501034.2843058. New log length: 1
2025-04-01 14:50:56,646 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501034.2843058, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.7128249406814575, 'message': 'BERTScore F1: 0.7128', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:51:36,438 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:39,970 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:39,976 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:39,976 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:39,979 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:48,684 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:51,442 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:51,444 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:51,445 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:51,447 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:53,018 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:53,019 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:53,027 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501113.027596 ---
2025-04-01 14:51:53,029 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:52:02,237 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:52:02,238 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:52:02,238 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:52:02,239 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:52:02,239 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:52:02,239 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:52:02,240 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:52:05,445 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:52:05,445 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:52:05,446 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:52:05,446 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:52:06,567 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:52:06,567 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:52:06,568 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:52:06,569 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:52:06,569 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501113.027596. New log length: 1
2025-04-01 14:52:06,570 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501113.027596, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.713367760181427, 'message': 'BERTScore F1: 0.7134', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:52:06,752 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:52:06,753 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:52:06,756 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,777 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 14:52:06,778 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,779 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:28,873 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:32,996 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:33,002 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 15:21:33,002 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 15:21:33,006 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:44,751 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:44,752 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:44,803 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743502904.8032691 ---
2025-04-01 15:21:44,805 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 15:22:09,153 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. In stage one, automated checks such as BERTScore and length constraints are employed. Stage two involves an AI Judge, specifica...
2025-04-01 15:22:09,156 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 15:22:09,156 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 15:22:09,156 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 15:22:09,157 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 15:22:09,157 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:22:09,170 - INFO - app.py:318 - Length check result: Passed=False, Details={'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:22:12,582 - INFO - app.py:329 - BERTScore calculated: Score=0.693, PassedThreshold=True
2025-04-01 15:22:12,583 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:22:12,583 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:22:12,584 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:22:13,856 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:22:13,857 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 15:22:13,857 - INFO - app.py:356 - Successfully parsed AI Judge response.
2025-04-01 15:22:13,857 - DEBUG - app.py:357 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:22:13,858 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:22:13,858 - INFO - app.py:379 - Evaluation results stored for timestamp 1743502904.8032691. New log length: 1
2025-04-01 15:22:13,858 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743502904.8032691, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6926636099815369, 'message': 'BERTScore F1: 0.6927', 'passed_threshold': True}, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 15:22:14,117 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:14,118 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:14,123 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,174 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:14,175 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,177 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:43,863 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:43,865 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:43,920 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,949 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:43,955 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,962 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:44,530 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,530 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,545 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,550 - INFO - app.py:494 - User submitted rating: 3 for log index 0 (Timestamp: 1743502904.8032691)
2025-04-01 15:22:44,671 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,671 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,675 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,679 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:44,680 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,682 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:06,223 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:09,502 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:09,507 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:09,507 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:09,509 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:19,174 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:22,191 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:22,195 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:22,195 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:22,197 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:23,794 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:23,795 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:23,797 - INFO - app.py:254 - --- Pipeline Run Button Clicked: Timestamp 1743504383.7977111 ---
2025-04-01 15:46:23,798 - INFO - app.py:265 - Initiating Generation Stage.
2025-04-01 15:46:43,111 - DEBUG - app.py:272 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage employs an AI Judge, speci...
2025-04-01 15:46:43,120 - INFO - app.py:289 - Checking format of generated output.
2025-04-01 15:46:43,121 - INFO - app.py:294 - Generated output format check passed.
2025-04-01 15:46:43,121 - DEBUG - app.py:298 - Successfully parsed generated data.
2025-04-01 15:46:43,121 - INFO - app.py:310 - Starting Evaluation Stages.
2025-04-01 15:46:43,122 - INFO - app.py:313 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:46:43,129 - INFO - app.py:321 - Length check result: Passed=False, Details={'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:46:46,487 - INFO - app.py:332 - BERTScore calculated: Score=0.671, PassedThreshold=True
2025-04-01 15:46:46,487 - INFO - app.py:340 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:46:46,487 - DEBUG - app.py:344 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:46:46,488 - INFO - app.py:349 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:46:47,813 - DEBUG - app.py:352 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:46:47,814 - INFO - app.py:358 - AI Judge call successful, parsing response.
2025-04-01 15:46:47,814 - INFO - app.py:362 - Successfully parsed AI Judge response.
2025-04-01 15:46:47,814 - DEBUG - app.py:363 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:46:47,816 - INFO - app.py:380 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:46:47,816 - INFO - app.py:390 - Evaluation results stored for timestamp 1743504383.7977111. New log length: 1
2025-04-01 15:46:47,817 - DEBUG - app.py:391 - Full evaluation results for current run: {'timestamp': 1743504383.7977111, 'generation_time': 19.322030782699585, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6705673336982727, 'message': 'BERTScore F1: 0.6706', 'passed_threshold': True}, 'stage1_time': 3.3644537925720215, 'stage2_time': 1.3285832405090332, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 3.0994415283203125e-06}
2025-04-01 15:46:47,999 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:48,000 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:48,003 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,028 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:46:48,029 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,031 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:09,721 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:09,722 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:09,737 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,752 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:09,755 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,761 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:11,041 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,041 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,051 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,055 - INFO - app.py:505 - User submitted rating: 3 for log index 0 (Timestamp: 1743504383.7977111)
2025-04-01 15:47:11,165 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,165 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,173 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,180 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:11,181 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,187 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:34,051 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:37,162 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:37,166 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:37,166 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:37,167 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:46,128 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:49,486 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:49,489 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:49,490 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:49,492 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:52,155 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:52,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:52,186 - INFO - app.py:263 - --- Pipeline Run Button Clicked: Timestamp 1743504712.186294 ---
2025-04-01 15:51:52,186 - INFO - app.py:274 - Initiating Generation Stage.
2025-04-01 15:52:08,718 - DEBUG - app.py:281 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. In the second stage, an AI Judge, specifica...
2025-04-01 15:52:08,719 - INFO - app.py:298 - Checking format of generated output.
2025-04-01 15:52:08,719 - INFO - app.py:303 - Generated output format check passed.
2025-04-01 15:52:08,719 - DEBUG - app.py:307 - Successfully parsed generated data.
2025-04-01 15:52:08,720 - INFO - app.py:319 - Starting Evaluation Stages.
2025-04-01 15:52:08,720 - INFO - app.py:322 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:52:08,725 - INFO - app.py:330 - Length check result: Passed=False, Details={'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:52:12,363 - INFO - app.py:341 - BERTScore calculated: Score=0.680, PassedThreshold=True
2025-04-01 15:52:12,364 - INFO - app.py:349 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:52:12,364 - DEBUG - app.py:353 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:52:12,365 - INFO - app.py:358 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:52:13,523 - DEBUG - app.py:361 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:52:13,524 - INFO - app.py:367 - AI Judge call successful, parsing response.
2025-04-01 15:52:13,525 - INFO - app.py:371 - Successfully parsed AI Judge response.
2025-04-01 15:52:13,525 - DEBUG - app.py:372 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:52:13,525 - INFO - app.py:389 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:52:13,526 - INFO - app.py:399 - Evaluation results stored for timestamp 1743504712.186294. New log length: 1
2025-04-01 15:52:13,526 - DEBUG - app.py:400 - Full evaluation results for current run: {'timestamp': 1743504712.186294, 'generation_time': 16.53201913833618, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6795765161514282, 'message': 'BERTScore F1: 0.6796', 'passed_threshold': True}, 'stage1_time': 3.644177198410034, 'stage2_time': 1.16099214553833, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 2.1457672119140625e-06}
2025-04-01 15:52:13,708 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:13,708 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:13,712 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,738 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:13,739 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,741 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,239 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,240 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,260 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,272 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:31,273 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,276 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,933 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,934 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,944 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,956 - INFO - app.py:514 - User submitted rating: 2 for log index 0 (Timestamp: 1743504712.186294)
2025-04-01 15:52:32,062 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:32,062 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:32,066 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,070 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:32,071 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,074 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
