2025-04-01 14:45:01,797 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:04,912 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:04,916 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:04,916 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:04,916 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:04,917 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:11,266 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:14,442 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:14,449 - DEBUG - app.py:219 - Initialized evaluation_log in session state.
2025-04-01 14:45:14,451 - DEBUG - app.py:222 - Initialized current_eval_data in session state.
2025-04-01 14:45:14,454 - INFO - app.py:499 - No evaluation logs yet in session state.
2025-04-01 14:45:14,455 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,769 - INFO - app.py:30 - --- Starting Streamlit App ---
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,770 - INFO - app.py:38 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,811 - INFO - app.py:234 - --- Pipeline Run Started: Timestamp 1743500718.811178 ---
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:18,812 - INFO - app.py:241 - Initiating Generation Stage.
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,974 - DEBUG - app.py:246 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:262 - Checking format of generated output.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,976 - INFO - app.py:268 - Generated output format check passed.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - DEBUG - app.py:279 - Successfully parsed and displayed generated data.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,978 - INFO - app.py:298 - Starting Evaluation Stages.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:30,979 - INFO - app.py:301 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,084 - DEBUG - app.py:306 - Performing length check.
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - INFO - app.py:309 - Length check result: Passed=True, Details={'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:31,085 - DEBUG - app.py:318 - Calculating BERTScore.
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,419 - INFO - app.py:326 - BERTScore calculated: Score=0.672, PassedThreshold=True
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - INFO - app.py:337 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,420 - DEBUG - app.py:343 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:34,421 - INFO - app.py:347 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,502 - DEBUG - app.py:350 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - INFO - app.py:356 - AI Judge call successful, parsing response.
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,504 - ERROR - app.py:364 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,505 - INFO - app.py:382 - Setting up Stage 3: Human Feedback.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - INFO - app.py:389 - Evaluation results stored for timestamp 1743500718.811178.
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,506 - DEBUG - app.py:390 - Full evaluation results: {'timestamp': 1743500718.811178, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.6721336245536804, 'message': 'BERTScore F1: 0.6721', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,509 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,513 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,514 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,516 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,518 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,522 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,609 - DEBUG - app.py:481 - Displaying details for Log ID 1
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - DEBUG - app.py:58 - Displaying evaluation results for timestamp: 1743500718.811178
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,610 - INFO - app.py:73 - Format check passed.
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,611 - INFO - app.py:88 - Length check passed. Details: {'summary_tokens': 98, 'flashcards_tokens': 114, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - INFO - app.py:102 - BERTScore passed threshold: 0.672
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,612 - ERROR - app.py:118 - AI Judge Error reported: AI Judge JSON missing required keys
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,613 - DEBUG - app.py:122 - Raw AI Judge Response on error: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,616 - INFO - app.py:188 - User Utility Rating not yet provided.
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:45:35,618 - INFO - app.py:515 - --- Streamlit App Execution Completed (End of Script) ---
2025-04-01 14:50:02,805 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:05,867 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:05,871 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:05,871 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:05,872 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:12,241 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:15,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:15,159 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:50:15,159 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:50:15,161 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:50:34,224 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:50:34,234 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:50:34,284 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501034.2843058 ---
2025-04-01 14:50:34,285 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:50:52,035 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video explains the AI evaluation pipeline, which consists of four stages. Stage one employs automated checks such as BERTScore and length constraints. Stage two utilizes an AI Judg...
2025-04-01 14:50:52,038 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:50:52,039 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:50:52,039 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:50:52,039 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:50:52,040 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:50:52,041 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:50:55,418 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:50:55,419 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:50:55,419 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:50:55,420 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:50:56,645 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:50:56,645 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:50:56,645 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:50:56,646 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:50:56,646 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501034.2843058. New log length: 1
2025-04-01 14:50:56,646 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501034.2843058, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 96, 'flashcards_tokens': 137, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.7128249406814575, 'message': 'BERTScore F1: 0.7128', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:51:36,438 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:39,970 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:39,976 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:39,976 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:39,979 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:48,684 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:51,442 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:51,444 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 14:51:51,445 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 14:51:51,447 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 14:51:53,018 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:51:53,019 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:51:53,027 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743501113.027596 ---
2025-04-01 14:51:53,029 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 14:52:02,237 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage involves an AI Judge, spec...
2025-04-01 14:52:02,238 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 14:52:02,238 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 14:52:02,239 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 14:52:02,239 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 14:52:02,239 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 14:52:02,240 - INFO - app.py:318 - Length check result: Passed=True, Details={'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 14:52:05,445 - INFO - app.py:329 - BERTScore calculated: Score=0.713, PassedThreshold=True
2025-04-01 14:52:05,445 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 14:52:05,446 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 14:52:05,446 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 14:52:06,567 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 14:52:06,567 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 14:52:06,568 - ERROR - app.py:360 - Failed to parse AI Judge response: AI Judge JSON missing required keys
2025-04-01 14:52:06,569 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 14:52:06,569 - INFO - app.py:379 - Evaluation results stored for timestamp 1743501113.027596. New log length: 1
2025-04-01 14:52:06,570 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743501113.027596, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 89, 'flashcards_tokens': 92, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.713367760181427, 'message': 'BERTScore F1: 0.7134', 'passed_threshold': True}, 'ai_judge_assessment': {'data': None, 'error': 'AI Judge JSON missing required keys', 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 14:52:06,752 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 14:52:06,753 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 14:52:06,756 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,777 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 14:52:06,778 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743501113.027596
2025-04-01 14:52:06,779 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:28,873 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:32,996 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:33,002 - DEBUG - app.py:240 - Initialized evaluation_log in session state.
2025-04-01 15:21:33,002 - DEBUG - app.py:244 - Initialized current_run_index in session state.
2025-04-01 15:21:33,006 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:21:44,751 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:21:44,752 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:21:44,803 - INFO - app.py:250 - --- Pipeline Run Button Clicked: Timestamp 1743502904.8032691 ---
2025-04-01 15:21:44,805 - INFO - app.py:261 - Initiating Generation Stage.
2025-04-01 15:22:09,153 - DEBUG - app.py:267 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. In stage one, automated checks such as BERTScore and length constraints are employed. Stage two involves an AI Judge, specifica...
2025-04-01 15:22:09,156 - INFO - app.py:282 - Checking format of generated output.
2025-04-01 15:22:09,156 - INFO - app.py:287 - Generated output format check passed.
2025-04-01 15:22:09,156 - DEBUG - app.py:291 - Successfully parsed generated data.
2025-04-01 15:22:09,157 - INFO - app.py:308 - Starting Evaluation Stages.
2025-04-01 15:22:09,157 - INFO - app.py:311 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:22:09,170 - INFO - app.py:318 - Length check result: Passed=False, Details={'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:22:12,582 - INFO - app.py:329 - BERTScore calculated: Score=0.693, PassedThreshold=True
2025-04-01 15:22:12,583 - INFO - app.py:335 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:22:12,583 - DEBUG - app.py:338 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:22:12,584 - INFO - app.py:343 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:22:13,856 - DEBUG - app.py:346 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:22:13,857 - INFO - app.py:352 - AI Judge call successful, parsing response.
2025-04-01 15:22:13,857 - INFO - app.py:356 - Successfully parsed AI Judge response.
2025-04-01 15:22:13,857 - DEBUG - app.py:357 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:22:13,858 - INFO - app.py:372 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:22:13,858 - INFO - app.py:379 - Evaluation results stored for timestamp 1743502904.8032691. New log length: 1
2025-04-01 15:22:13,858 - DEBUG - app.py:380 - Full evaluation results for current run: {'timestamp': 1743502904.8032691, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 121, 'flashcards_tokens': 157, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6926636099815369, 'message': 'BERTScore F1: 0.6927', 'passed_threshold': True}, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None}
2025-04-01 15:22:14,117 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:14,118 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:14,123 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,174 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:14,175 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:14,177 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:43,863 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:43,865 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:43,920 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,949 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:43,955 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:43,962 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:22:44,530 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,530 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,545 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,550 - INFO - app.py:494 - User submitted rating: 3 for log index 0 (Timestamp: 1743502904.8032691)
2025-04-01 15:22:44,671 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:22:44,671 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:22:44,675 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,679 - DEBUG - app.py:556 - Displaying details for Log ID 1
2025-04-01 15:22:44,680 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743502904.8032691
2025-04-01 15:22:44,682 - INFO - app.py:583 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:06,223 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:09,502 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:09,507 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:09,507 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:09,509 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:19,174 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:22,191 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:22,195 - DEBUG - app.py:244 - Initialized evaluation_log in session state.
2025-04-01 15:46:22,195 - DEBUG - app.py:248 - Initialized current_run_index in session state.
2025-04-01 15:46:22,197 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:46:23,794 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:23,795 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:23,797 - INFO - app.py:254 - --- Pipeline Run Button Clicked: Timestamp 1743504383.7977111 ---
2025-04-01 15:46:23,798 - INFO - app.py:265 - Initiating Generation Stage.
2025-04-01 15:46:43,111 - DEBUG - app.py:272 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. The second stage employs an AI Judge, speci...
2025-04-01 15:46:43,120 - INFO - app.py:289 - Checking format of generated output.
2025-04-01 15:46:43,121 - INFO - app.py:294 - Generated output format check passed.
2025-04-01 15:46:43,121 - DEBUG - app.py:298 - Successfully parsed generated data.
2025-04-01 15:46:43,121 - INFO - app.py:310 - Starting Evaluation Stages.
2025-04-01 15:46:43,122 - INFO - app.py:313 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:46:43,129 - INFO - app.py:321 - Length check result: Passed=False, Details={'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:46:46,487 - INFO - app.py:332 - BERTScore calculated: Score=0.671, PassedThreshold=True
2025-04-01 15:46:46,487 - INFO - app.py:340 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:46:46,487 - DEBUG - app.py:344 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:46:46,488 - INFO - app.py:349 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:46:47,813 - DEBUG - app.py:352 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:46:47,814 - INFO - app.py:358 - AI Judge call successful, parsing response.
2025-04-01 15:46:47,814 - INFO - app.py:362 - Successfully parsed AI Judge response.
2025-04-01 15:46:47,814 - DEBUG - app.py:363 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:46:47,816 - INFO - app.py:380 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:46:47,816 - INFO - app.py:390 - Evaluation results stored for timestamp 1743504383.7977111. New log length: 1
2025-04-01 15:46:47,817 - DEBUG - app.py:391 - Full evaluation results for current run: {'timestamp': 1743504383.7977111, 'generation_time': 19.322030782699585, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 126, 'flashcards_tokens': 171, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6705673336982727, 'message': 'BERTScore F1: 0.6706', 'passed_threshold': True}, 'stage1_time': 3.3644537925720215, 'stage2_time': 1.3285832405090332, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 3.0994415283203125e-06}
2025-04-01 15:46:47,999 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:46:48,000 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:46:48,003 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,028 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:46:48,029 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:46:48,031 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:09,721 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:09,722 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:09,737 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,752 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:09,755 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:09,761 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:47:11,041 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,041 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,051 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,055 - INFO - app.py:505 - User submitted rating: 3 for log index 0 (Timestamp: 1743504383.7977111)
2025-04-01 15:47:11,165 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:47:11,165 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:47:11,173 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,180 - DEBUG - app.py:567 - Displaying details for Log ID 1
2025-04-01 15:47:11,181 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504383.7977111
2025-04-01 15:47:11,187 - INFO - app.py:594 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:34,051 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:37,162 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:37,166 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:37,166 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:37,167 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:46,128 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:49,486 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:49,489 - DEBUG - app.py:253 - Initialized evaluation_log in session state.
2025-04-01 15:51:49,490 - DEBUG - app.py:257 - Initialized current_run_index in session state.
2025-04-01 15:51:49,492 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:51:52,155 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:51:52,156 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:51:52,186 - INFO - app.py:263 - --- Pipeline Run Button Clicked: Timestamp 1743504712.186294 ---
2025-04-01 15:51:52,186 - INFO - app.py:274 - Initiating Generation Stage.
2025-04-01 15:52:08,718 - DEBUG - app.py:281 - Generator Raw Response Snippet: {
  "summary": "The video describes a four-stage AI evaluation pipeline. The first stage includes automated checks such as BERTScore and length constraints. In the second stage, an AI Judge, specifica...
2025-04-01 15:52:08,719 - INFO - app.py:298 - Checking format of generated output.
2025-04-01 15:52:08,719 - INFO - app.py:303 - Generated output format check passed.
2025-04-01 15:52:08,719 - DEBUG - app.py:307 - Successfully parsed generated data.
2025-04-01 15:52:08,720 - INFO - app.py:319 - Starting Evaluation Stages.
2025-04-01 15:52:08,720 - INFO - app.py:322 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 15:52:08,725 - INFO - app.py:330 - Length check result: Passed=False, Details={'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 15:52:12,363 - INFO - app.py:341 - BERTScore calculated: Score=0.680, PassedThreshold=True
2025-04-01 15:52:12,364 - INFO - app.py:349 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 15:52:12,364 - DEBUG - app.py:353 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 15:52:12,365 - INFO - app.py:358 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 15:52:13,523 - DEBUG - app.py:361 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 15:52:13,524 - INFO - app.py:367 - AI Judge call successful, parsing response.
2025-04-01 15:52:13,525 - INFO - app.py:371 - Successfully parsed AI Judge response.
2025-04-01 15:52:13,525 - DEBUG - app.py:372 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 15:52:13,525 - INFO - app.py:389 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 15:52:13,526 - INFO - app.py:399 - Evaluation results stored for timestamp 1743504712.186294. New log length: 1
2025-04-01 15:52:13,526 - DEBUG - app.py:400 - Full evaluation results for current run: {'timestamp': 1743504712.186294, 'generation_time': 16.53201913833618, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 119, 'flashcards_tokens': 161, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.6795765161514282, 'message': 'BERTScore F1: 0.6796', 'passed_threshold': True}, 'stage1_time': 3.644177198410034, 'stage2_time': 1.16099214553833, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 2.1457672119140625e-06}
2025-04-01 15:52:13,708 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:13,708 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:13,712 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,738 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:13,739 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:13,741 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,239 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,240 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,260 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,272 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:31,273 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,276 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:52:31,933 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:31,934 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:31,944 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:31,956 - INFO - app.py:514 - User submitted rating: 2 for log index 0 (Timestamp: 1743504712.186294)
2025-04-01 15:52:32,062 - INFO - app.py:32 - --- Starting Streamlit App ---
2025-04-01 15:52:32,062 - INFO - app.py:39 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:52:32,066 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,070 - DEBUG - app.py:576 - Displaying details for Log ID 1
2025-04-01 15:52:32,071 - DEBUG - app.py:62 - Displaying evaluation results for timestamp: 1743504712.186294
2025-04-01 15:52:32,074 - INFO - app.py:603 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:02,379 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:05,324 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:57:05,330 - DEBUG - app.py:312 - Initialized evaluation_log in session state.
2025-04-01 15:57:05,330 - DEBUG - app.py:316 - Initialized current_run_index in session state.
2025-04-01 15:57:05,331 - INFO - app.py:662 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:12,526 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:16,436 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 15:57:16,450 - DEBUG - app.py:312 - Initialized evaluation_log in session state.
2025-04-01 15:57:16,451 - DEBUG - app.py:316 - Initialized current_run_index in session state.
2025-04-01 15:57:16,474 - INFO - app.py:662 - --- Streamlit App Re-Render Complete ---
2025-04-01 15:57:22,586 - INFO - app.py:36 - --- Starting Streamlit App ---
2025-04-01 15:57:22,588 - INFO - app.py:72 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:03,038 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:06,566 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:06,576 - DEBUG - app.py:277 - Initialized evaluation_log in session state.
2025-04-01 16:03:06,576 - DEBUG - app.py:281 - Initialized current_run_index in session state.
2025-04-01 16:03:06,578 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:15,086 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:18,159 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:18,177 - DEBUG - app.py:277 - Initialized evaluation_log in session state.
2025-04-01 16:03:18,178 - DEBUG - app.py:281 - Initialized current_run_index in session state.
2025-04-01 16:03:18,197 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:33,243 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:33,245 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:03:33,272 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:03:35,334 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:03:35,335 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:31,951 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:35,086 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:35,092 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:05:35,092 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:05:35,096 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:40,978 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:43,984 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:43,988 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:05:43,989 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:05:43,991 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:55,993 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:55,994 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:05:56,030 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:05:57,182 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:05:57,182 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:17,883 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:20,892 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:20,898 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:07:20,898 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:07:20,900 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:28,883 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:31,941 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:31,944 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:07:31,945 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:07:31,947 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:35,979 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:35,981 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:35,990 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:07:36,865 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:07:36,865 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:07:45,593 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:41,732 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:44,770 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:44,775 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:08:44,775 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:08:44,777 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:52,713 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:55,833 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:55,837 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:08:55,837 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:08:55,839 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:08:58,979 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:08:58,980 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:08:59,030 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:09:00,548 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:09:00,548 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:09:04,234 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743505744.234179 ---
2025-04-01 16:09:04,235 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:09:16,168 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video provides a series of guided stretching exercises focusing on various parts of the body, including the hips, spine, shoulders, and legs. Each stretch is held for a specific du...
2025-04-01 16:09:16,170 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:09:16,171 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:09:16,171 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:09:16,171 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:09:16,172 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:09:16,182 - INFO - app.py:351 - Length check result: Passed=True, Details={'summary_tokens': 138, 'flashcards_tokens': 128, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 16:09:20,218 - INFO - app.py:362 - BERTScore calculated: Score=-0.455, PassedThreshold=False
2025-04-01 16:09:20,219 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:09:20,219 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:09:20,219 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:09:22,301 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:09:22,301 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:09:22,302 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:09:22,302 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:09:22,302 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:09:22,302 - INFO - app.py:420 - Evaluation results stored for timestamp 1743505744.234179. New log length: 1
2025-04-01 16:09:22,302 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743505744.234179, 'generation_time': 11.935593128204346, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 138, 'flashcards_tokens': 128, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': -0.45474475622177124, 'message': 'BERTScore F1: -0.4547', 'passed_threshold': False}, 'stage1_time': 4.046546936035156, 'stage2_time': 2.08321475982666, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 0.0}
2025-04-01 16:09:22,470 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:09:22,470 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:09:22,473 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505744.234179
2025-04-01 16:09:22,501 - DEBUG - app.py:597 - Displaying details for Log ID 1
2025-04-01 16:09:22,502 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505744.234179
2025-04-01 16:09:22,504 - INFO - app.py:624 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:06,160 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:09,514 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:09,520 - DEBUG - app.py:275 - Initialized evaluation_log in session state.
2025-04-01 16:11:09,520 - DEBUG - app.py:279 - Initialized current_run_index in session state.
2025-04-01 16:11:09,521 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:20,121 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:24,054 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:24,058 - DEBUG - app.py:275 - Initialized evaluation_log in session state.
2025-04-01 16:11:24,059 - DEBUG - app.py:279 - Initialized current_run_index in session state.
2025-04-01 16:11:24,063 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:28,902 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:28,905 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:28,969 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:11:30,429 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:30,430 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:34,119 - INFO - app.py:284 - --- Pipeline Run Initiated: Timestamp 1743505894.119435 ---
2025-04-01 16:11:34,120 - INFO - app.py:298 - Initiating Generation Stage.
2025-04-01 16:11:46,283 - DEBUG - app.py:305 - Generator Raw Response Snippet: {
  "summary": "The discussion focuses on the anterior midcingulate cortex, a brain area that enlarges when individuals engage in activities they dislike, such as exercise or dieting. This enlargement...
2025-04-01 16:11:46,284 - INFO - app.py:322 - Checking format of generated output.
2025-04-01 16:11:46,285 - INFO - app.py:327 - Generated output format check passed.
2025-04-01 16:11:46,285 - DEBUG - app.py:331 - Successfully parsed generated data.
2025-04-01 16:11:46,285 - INFO - app.py:343 - Starting Evaluation Stages.
2025-04-01 16:11:46,285 - INFO - app.py:346 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:11:46,300 - INFO - app.py:354 - Length check result: Passed=False, Details={'summary_tokens': 140, 'flashcards_tokens': 156, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 16:11:50,415 - INFO - app.py:365 - BERTScore calculated: Score=-0.353, PassedThreshold=False
2025-04-01 16:11:50,416 - INFO - app.py:373 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:11:50,416 - DEBUG - app.py:377 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:11:50,416 - INFO - app.py:382 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:11:51,950 - DEBUG - app.py:385 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:11:51,950 - INFO - app.py:391 - AI Judge call successful, parsing response.
2025-04-01 16:11:51,951 - INFO - app.py:395 - Successfully parsed AI Judge response.
2025-04-01 16:11:51,951 - DEBUG - app.py:396 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:11:51,952 - INFO - app.py:413 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:11:51,953 - INFO - app.py:423 - Evaluation results stored for timestamp 1743505894.119435. New log length: 1
2025-04-01 16:11:51,953 - DEBUG - app.py:424 - Full evaluation results for current run: {'timestamp': 1743505894.119435, 'generation_time': 12.163750171661377, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 140, 'flashcards_tokens': 156, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': -0.3531835079193115, 'message': 'BERTScore F1: -0.3532', 'passed_threshold': False}, 'stage1_time': 4.130232095718384, 'stage2_time': 1.5359771251678467, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 2.1457672119140625e-06}
2025-04-01 16:11:52,162 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:11:52,163 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:11:52,166 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505894.119435
2025-04-01 16:11:52,196 - DEBUG - app.py:600 - Displaying details for Log ID 1
2025-04-01 16:11:52,196 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743505894.119435
2025-04-01 16:11:52,198 - INFO - app.py:627 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:14:23,388 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:14:26,533 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:14:26,539 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:14:26,539 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:14:26,544 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:14:34,239 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:14:37,082 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:14:37,086 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:14:37,087 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:14:37,089 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:15:04,344 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:04,347 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:04,370 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:15:05,729 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:05,729 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:11,183 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743506111.1835551 ---
2025-04-01 16:15:11,184 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:15:24,199 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video transcript describes a series of stretching exercises focusing on different body parts including the hamstrings, quads, hips, and groin. Key stretches demonstrated include dy...
2025-04-01 16:15:24,200 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:15:24,200 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:15:24,201 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:15:24,201 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:15:24,201 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:15:24,208 - INFO - app.py:351 - Length check result: Passed=True, Details={'summary_tokens': 115, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-01 16:15:28,480 - INFO - app.py:362 - BERTScore calculated: Score=-0.477, PassedThreshold=False
2025-04-01 16:15:28,482 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:15:28,482 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:15:28,482 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:15:30,404 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:15:30,404 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:15:30,404 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:15:30,404 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:15:30,405 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:15:30,405 - INFO - app.py:420 - Evaluation results stored for timestamp 1743506111.1835551. New log length: 1
2025-04-01 16:15:30,405 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743506111.1835551, 'generation_time': 13.015744924545288, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 115, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': -0.4767054319381714, 'message': 'BERTScore F1: -0.4767', 'passed_threshold': False}, 'stage1_time': 4.280241250991821, 'stage2_time': 1.922940731048584, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 9.5367431640625e-07}
2025-04-01 16:15:30,581 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:15:30,582 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:15:30,585 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506111.1835551
2025-04-01 16:15:30,614 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-01 16:15:30,615 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506111.1835551
2025-04-01 16:15:30,617 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:03,835 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:06,959 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:06,965 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:17:06,965 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:17:06,967 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:14,619 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:17,779 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:17,785 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:17:17,785 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:17:17,788 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:20,696 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:20,697 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:20,732 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:22,267 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:22,268 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:22,289 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:32,122 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:32,125 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:32,165 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:17:32,347 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:17:32,347 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:17:33,445 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:18:48,764 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:18:51,776 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:18:51,781 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:18:51,781 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:18:51,784 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:18:58,433 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:01,818 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:01,829 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:19:01,830 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:19:01,832 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:19:15,544 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:15,547 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:15,572 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:19:17,112 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:19:17,112 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:19:18,169 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:38,309 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:41,416 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:41,428 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:21:41,428 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:21:41,430 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:47,570 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:50,424 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:50,429 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-01 16:21:50,429 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-01 16:21:50,431 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:53,762 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:53,763 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:53,772 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-01 16:21:54,825 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:21:54,828 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:21:55,850 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743506515.850836 ---
2025-04-01 16:21:55,851 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-01 16:22:21,704 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video demonstrates a series of dynamic and static stretching exercises targeting various muscle groups. It starts with a dynamic lunge to hamstring stretch, transitioning between b...
2025-04-01 16:22:21,707 - INFO - app.py:319 - Checking format of generated output.
2025-04-01 16:22:21,707 - INFO - app.py:324 - Generated output format check passed.
2025-04-01 16:22:21,707 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-01 16:22:21,708 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-01 16:22:21,708 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-01 16:22:21,719 - INFO - app.py:351 - Length check result: Passed=False, Details={'summary_tokens': 170, 'flashcards_tokens': 205, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-01 16:22:25,650 - INFO - app.py:362 - BERTScore calculated: Score=-0.175, PassedThreshold=False
2025-04-01 16:22:25,652 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-01 16:22:25,652 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-01 16:22:25,652 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-01 16:22:26,957 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-01 16:22:26,957 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-01 16:22:26,958 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-01 16:22:26,958 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-01 16:22:26,959 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-01 16:22:26,960 - INFO - app.py:420 - Evaluation results stored for timestamp 1743506515.850836. New log length: 1
2025-04-01 16:22:26,960 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743506515.850836, 'generation_time': 25.85506820678711, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 170, 'flashcards_tokens': 205, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': -0.17490310966968536, 'message': 'BERTScore F1: -0.1749', 'passed_threshold': False}, 'stage1_time': 3.9433281421661377, 'stage2_time': 1.3073318004608154, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 1.9073486328125e-06}
2025-04-01 16:22:27,138 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-01 16:22:27,139 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-01 16:22:27,142 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506515.850836
2025-04-01 16:22:27,169 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-01 16:22:27,169 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743506515.850836
2025-04-01 16:22:27,171 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:34:42,021 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:34:45,239 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:34:45,244 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-02 13:34:45,244 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-02 13:34:45,249 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:34:54,688 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:34:57,916 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:34:57,923 - DEBUG - app.py:272 - Initialized evaluation_log in session state.
2025-04-02 13:34:57,924 - DEBUG - app.py:276 - Initialized current_run_index in session state.
2025-04-02 13:34:57,968 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:37:15,904 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:37:15,907 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:37:15,966 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:37:17,757 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:37:17,758 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:37:18,872 - INFO - app.py:281 - --- Pipeline Run Initiated: Timestamp 1743583038.872088 ---
2025-04-02 13:37:18,872 - INFO - app.py:295 - Initiating Generation Stage.
2025-04-02 13:37:35,170 - DEBUG - app.py:302 - Generator Raw Response Snippet: {
  "summary": "The video discusses the anterior midcingulate cortex, a brain structure associated with willpower and the ability to perform tasks one does not want to do. It is noted that this area e...
2025-04-02 13:37:35,173 - INFO - app.py:319 - Checking format of generated output.
2025-04-02 13:37:35,173 - INFO - app.py:324 - Generated output format check passed.
2025-04-02 13:37:35,173 - DEBUG - app.py:328 - Successfully parsed generated data.
2025-04-02 13:37:35,174 - INFO - app.py:340 - Starting Evaluation Stages.
2025-04-02 13:37:35,174 - INFO - app.py:343 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 13:37:35,193 - INFO - app.py:351 - Length check result: Passed=False, Details={'summary_tokens': 157, 'flashcards_tokens': 153, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-02 13:37:39,607 - INFO - app.py:362 - BERTScore calculated: Score=-0.072, PassedThreshold=False
2025-04-02 13:37:39,608 - INFO - app.py:370 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 13:37:39,608 - DEBUG - app.py:374 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 13:37:39,609 - INFO - app.py:379 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 13:37:41,184 - DEBUG - app.py:382 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 13:37:41,185 - INFO - app.py:388 - AI Judge call successful, parsing response.
2025-04-02 13:37:41,185 - INFO - app.py:392 - Successfully parsed AI Judge response.
2025-04-02 13:37:41,185 - DEBUG - app.py:393 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 13:37:41,185 - INFO - app.py:410 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 13:37:41,185 - INFO - app.py:420 - Evaluation results stored for timestamp 1743583038.872088. New log length: 1
2025-04-02 13:37:41,185 - DEBUG - app.py:421 - Full evaluation results for current run: {'timestamp': 1743583038.872088, 'generation_time': 16.300368785858154, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 157, 'flashcards_tokens': 153, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': -0.07238689810037613, 'message': 'BERTScore F1: -0.0724', 'passed_threshold': False}, 'stage1_time': 4.433567047119141, 'stage2_time': 1.576781988143921, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 5.0067901611328125e-06}
2025-04-02 13:37:41,352 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:37:41,352 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:37:41,356 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:37:41,389 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-02 13:37:41,390 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:37:41,392 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:38:17,916 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:38:17,918 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:38:17,957 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:38:17,993 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-02 13:38:17,995 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:38:17,999 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:38:19,480 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:38:19,480 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:38:19,487 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:38:19,496 - INFO - app.py:543 - User submitted rating: 2 for log index 0 (Timestamp: 1743583038.872088)
2025-04-02 13:38:19,683 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:38:19,683 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:38:19,700 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:38:19,727 - DEBUG - app.py:605 - Displaying details for Log ID 1
2025-04-02 13:38:19,730 - DEBUG - app.py:66 - Displaying evaluation results for timestamp: 1743583038.872088
2025-04-02 13:38:19,746 - INFO - app.py:632 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:53:47,796 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:53:50,847 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:53:50,852 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 13:53:50,852 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 13:53:50,854 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:53:59,332 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:54:03,832 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:54:03,846 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 13:54:03,846 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 13:54:03,857 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:54:10,875 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:54:10,879 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:54:10,944 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 13:54:11,847 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:54:11,847 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:54:13,076 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743584053.0766752 ---
2025-04-02 13:54:13,077 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 13:54:33,579 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
  "summary": "The video discusses the concept of building a Minimum Viable Product (MVP) for startups, emphasizing the importance of launching quickly, learning from initial customer feedback, and i...
2025-04-02 13:54:33,581 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 13:54:33,582 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 13:54:33,582 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 13:54:33,582 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 13:54:33,582 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 13:54:33,588 - INFO - app.py:350 - Length check result: Passed=False, Details={'summary_tokens': 143, 'flashcards_tokens': 195, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-02 13:54:38,253 - INFO - app.py:361 - BERTScore calculated: Score=0.798, PassedThreshold=True
2025-04-02 13:54:38,256 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 13:54:38,257 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 13:54:38,260 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 13:54:39,705 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 13:54:39,705 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 13:54:39,705 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 13:54:39,705 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 13:54:39,705 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 13:54:39,706 - INFO - app.py:419 - Evaluation results stored for timestamp 1743584053.0766752. New log length: 1
2025-04-02 13:54:39,706 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743584053.0766752, 'generation_time': 20.50394105911255, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 143, 'flashcards_tokens': 195, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.7982509732246399, 'message': 'BERTScore F1: 0.7983', 'passed_threshold': True}, 'stage1_time': 4.674087762832642, 'stage2_time': 1.448225975036621, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 4.76837158203125e-06}
2025-04-02 13:54:39,872 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 13:54:39,872 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 13:54:39,876 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584053.0766752
2025-04-02 13:54:39,911 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 13:54:39,912 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584053.0766752
2025-04-02 13:54:39,914 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:03:51,532 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:03:54,673 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:03:54,681 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:03:54,681 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:03:54,683 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:04:20,108 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:04:23,239 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:04:23,284 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:04:23,287 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:04:23,294 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:04:28,300 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:04:28,303 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:04:28,368 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:04:29,774 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:04:29,774 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:04:30,744 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743584670.744072 ---
2025-04-02 14:04:30,744 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 14:04:47,259 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
  "summary": "The video discusses the concept of building a Minimum Viable Product (MVP) for startups, emphasizing the importance of launching a basic version of a product quickly to learn from real...
2025-04-02 14:04:47,263 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 14:04:47,264 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 14:04:47,264 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 14:04:47,264 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 14:04:47,265 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 14:04:47,271 - INFO - app.py:350 - Length check result: Passed=False, Details={'summary_tokens': 176, 'flashcards_tokens': 195, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-02 14:04:51,826 - INFO - app.py:361 - BERTScore calculated: Score=0.798, PassedThreshold=True
2025-04-02 14:04:51,827 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 14:04:51,827 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 14:04:51,828 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 14:04:53,571 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 14:04:53,572 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 14:04:53,573 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 14:04:53,573 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 14:04:53,574 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 14:04:53,574 - INFO - app.py:419 - Evaluation results stored for timestamp 1743584670.744072. New log length: 1
2025-04-02 14:04:53,575 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743584670.744072, 'generation_time': 16.518878936767578, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 176, 'flashcards_tokens': 195, 'summary_limit': 500, 'flashcards_limit': 150, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.7977903485298157, 'message': 'BERTScore F1: 0.7978', 'passed_threshold': True}, 'stage1_time': 4.562280893325806, 'stage2_time': 1.7465360164642334, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 1.3113021850585938e-05}
2025-04-02 14:04:53,769 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:04:53,769 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:04:53,773 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584670.744072
2025-04-02 14:04:53,806 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:04:53,807 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584670.744072
2025-04-02 14:04:53,809 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:06:18,437 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:06:21,418 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:06:21,429 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:06:21,429 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:06:21,430 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:06:30,254 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:06:34,308 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:06:34,343 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:06:34,345 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:06:34,372 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:06:38,432 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:06:38,432 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:06:38,468 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:06:39,305 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:06:39,307 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:06:40,271 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743584800.271049 ---
2025-04-02 14:06:40,280 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 14:07:13,295 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
  "summary": "The video discusses the concept of a Minimum Viable Product (MVP) and emphasizes the importance of launching a product quickly to facilitate learning through user interaction. The spea...
2025-04-02 14:07:13,298 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 14:07:13,298 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 14:07:13,298 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 14:07:13,298 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 14:07:13,299 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 14:07:13,308 - INFO - app.py:350 - Length check result: Passed=False, Details={'summary_tokens': 200, 'flashcards_tokens': 261, 'summary_limit': 500, 'flashcards_limit': 200, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}
2025-04-02 14:07:17,579 - INFO - app.py:361 - BERTScore calculated: Score=0.799, PassedThreshold=True
2025-04-02 14:07:17,581 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 14:07:17,581 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 14:07:17,585 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 14:07:19,019 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 14:07:19,020 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 14:07:19,020 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 14:07:19,020 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 14:07:19,021 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 14:07:19,021 - INFO - app.py:419 - Evaluation results stored for timestamp 1743584800.271049. New log length: 1
2025-04-02 14:07:19,021 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743584800.271049, 'generation_time': 33.017104148864746, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': False, 'details': {'summary_tokens': 200, 'flashcards_tokens': 261, 'summary_limit': 500, 'flashcards_limit': 200, 'summary_ok': True, 'flashcards_ok': False, 'overall_ok': False}}, 'bert_score': {'score': 0.7985404133796692, 'message': 'BERTScore F1: 0.7985', 'passed_threshold': True}, 'stage1_time': 4.2818849086761475, 'stage2_time': 1.4396209716796875, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 6.198883056640625e-06}
2025-04-02 14:07:19,207 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:07:19,207 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:07:19,212 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584800.271049
2025-04-02 14:07:19,244 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:07:19,245 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743584800.271049
2025-04-02 14:07:19,247 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:09:24,045 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:09:27,254 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:09:27,259 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:09:27,259 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:09:27,260 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:09:35,329 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:09:39,129 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:09:39,133 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:09:39,134 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:09:39,143 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:11:05,692 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:05,695 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:05,741 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:11:07,206 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:07,206 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:08,329 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743585068.329209 ---
2025-04-02 14:11:08,329 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 14:11:24,474 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
    "summary": "The video provides a guided session on various stretching exercises focusing on different parts of the body, each held for specific durations to enhance mobility and flexibility. The...
2025-04-02 14:11:24,476 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 14:11:24,477 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 14:11:24,477 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 14:11:24,477 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 14:11:24,478 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 14:11:24,498 - INFO - app.py:350 - Length check result: Passed=True, Details={'summary_tokens': 194, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-02 14:11:31,402 - INFO - app.py:361 - BERTScore calculated: Score=0.790, PassedThreshold=True
2025-04-02 14:11:31,405 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 14:11:31,405 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 14:11:31,406 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 14:11:32,635 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 14:11:32,635 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 14:11:32,636 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 14:11:32,636 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 14:11:32,637 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 14:11:32,637 - INFO - app.py:419 - Evaluation results stored for timestamp 1743585068.329209. New log length: 1
2025-04-02 14:11:32,637 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743585068.329209, 'generation_time': 16.147245168685913, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 194, 'flashcards_tokens': 136, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.7901723384857178, 'message': 'BERTScore F1: 0.7902', 'passed_threshold': True}, 'stage1_time': 6.927273988723755, 'stage2_time': 1.2315821647644043, 'ai_judge_assessment': {'data': {'completeness_score': 5, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 5, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 3.0994415283203125e-06}
2025-04-02 14:11:33,175 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:33,176 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:33,180 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:33,225 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:11:33,226 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:33,228 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:11:55,076 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:55,080 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:55,130 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:55,142 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:11:55,144 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:55,149 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:11:56,288 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:56,288 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:56,305 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:56,309 - INFO - app.py:542 - User submitted rating: 4 for log index 0 (Timestamp: 1743585068.329209)
2025-04-02 14:11:56,413 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:11:56,413 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:11:56,416 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:56,421 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:11:56,422 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585068.329209
2025-04-02 14:11:56,424 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:17:09,913 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:13,120 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:13,126 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:17:13,126 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:17:13,127 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:17:16,639 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:19,187 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:19,191 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:17:19,191 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:17:19,193 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:17:23,743 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:26,493 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:26,498 - DEBUG - app.py:271 - Initialized evaluation_log in session state.
2025-04-02 14:17:26,498 - DEBUG - app.py:275 - Initialized current_run_index in session state.
2025-04-02 14:17:26,509 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:17:32,096 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:32,098 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:32,146 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:17:34,224 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:34,224 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:35,329 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743585455.3297741 ---
2025-04-02 14:17:35,330 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 14:17:39,049 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
    "summary": "The transcript discusses the anterior midcingulate cortex, a brain area that grows when individuals push themselves to do things they don't want to do. It is smaller in obese individ...
2025-04-02 14:17:39,050 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 14:17:39,050 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 14:17:39,051 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 14:17:39,051 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 14:17:39,051 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 14:17:39,056 - INFO - app.py:350 - Length check result: Passed=True, Details={'summary_tokens': 90, 'flashcards_tokens': 173, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-02 14:17:43,023 - INFO - app.py:361 - BERTScore calculated: Score=0.823, PassedThreshold=True
2025-04-02 14:17:43,024 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 14:17:43,024 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 14:17:43,025 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 14:17:44,277 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 14:17:44,278 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 14:17:44,278 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 14:17:44,278 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 14:17:44,279 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 14:17:44,280 - INFO - app.py:419 - Evaluation results stored for timestamp 1743585455.3297741. New log length: 1
2025-04-02 14:17:44,281 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743585455.3297741, 'generation_time': 3.719507932662964, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 90, 'flashcards_tokens': 173, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.8227663040161133, 'message': 'BERTScore F1: 0.8228', 'passed_threshold': True}, 'stage1_time': 3.972990036010742, 'stage2_time': 1.2546906471252441, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 7.867813110351562e-06}
2025-04-02 14:17:44,451 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:17:44,451 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:17:44,454 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585455.3297741
2025-04-02 14:17:44,494 - DEBUG - app.py:604 - Displaying details for Log ID 1
2025-04-02 14:17:44,494 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585455.3297741
2025-04-02 14:17:44,496 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-02 14:26:55,307 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:26:55,311 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:26:55,404 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585455.3297741
2025-04-02 14:26:55,473 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:26:55,473 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:26:56,697 - INFO - app.py:280 - --- Pipeline Run Initiated: Timestamp 1743586016.697772 ---
2025-04-02 14:26:56,698 - INFO - app.py:294 - Initiating Generation Stage.
2025-04-02 14:27:00,460 - DEBUG - app.py:301 - Generator Raw Response Snippet: {
    "summary": "The video demonstrates a series of dynamic stretches focusing on lunges, hamstring, bridge, quad, and groin stretches. It emphasizes proper form, alignment, and active engagement dur...
2025-04-02 14:27:00,463 - INFO - app.py:318 - Checking format of generated output.
2025-04-02 14:27:00,463 - INFO - app.py:323 - Generated output format check passed.
2025-04-02 14:27:00,464 - DEBUG - app.py:327 - Successfully parsed generated data.
2025-04-02 14:27:00,464 - INFO - app.py:339 - Starting Evaluation Stages.
2025-04-02 14:27:00,464 - INFO - app.py:342 - Running Stage 1: Length & BERTScore Checks.
2025-04-02 14:27:00,485 - INFO - app.py:350 - Length check result: Passed=True, Details={'summary_tokens': 57, 'flashcards_tokens': 184, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}
2025-04-02 14:27:04,854 - INFO - app.py:361 - BERTScore calculated: Score=0.786, PassedThreshold=True
2025-04-02 14:27:04,856 - INFO - app.py:369 - Initiating Stage 2: AI Judge Assessment.
2025-04-02 14:27:04,856 - DEBUG - app.py:373 - Decision to run AI Judge: True (FormatOK=True)
2025-04-02 14:27:04,857 - INFO - app.py:378 - Calling AI Judge: gemini-1.5-flash-latest
2025-04-02 14:27:06,707 - DEBUG - app.py:381 - AI Judge Raw Response Snippet: {"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}...
2025-04-02 14:27:06,709 - INFO - app.py:387 - AI Judge call successful, parsing response.
2025-04-02 14:27:06,709 - INFO - app.py:391 - Successfully parsed AI Judge response.
2025-04-02 14:27:06,709 - DEBUG - app.py:392 - Parsed AI Judge Data: {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}
2025-04-02 14:27:06,710 - INFO - app.py:409 - Setting up Stage 3: Human Feedback placeholder.
2025-04-02 14:27:06,711 - INFO - app.py:419 - Evaluation results stored for timestamp 1743586016.697772. New log length: 2
2025-04-02 14:27:06,711 - DEBUG - app.py:420 - Full evaluation results for current run: {'timestamp': 1743586016.697772, 'generation_time': 3.764833688735962, 'format_check': {'passed': True, 'message': 'Valid format'}, 'length_check': {'passed': True, 'details': {'summary_tokens': 57, 'flashcards_tokens': 184, 'summary_limit': 500, 'flashcards_limit': 250, 'summary_ok': True, 'flashcards_ok': True, 'overall_ok': True}}, 'bert_score': {'score': 0.7864044308662415, 'message': 'BERTScore F1: 0.7864', 'passed_threshold': True}, 'stage1_time': 4.391304969787598, 'stage2_time': 1.853891134262085, 'ai_judge_assessment': {'data': {'completeness_score': 4, 'relevance_score': 5, 'clarity_score': 5, 'accuracy_assessment': {'contains_inaccuracies': False, 'explanation': ''}, 'optional_overall_notes': ''}, 'error': None, 'raw_response': '{"contains_inaccuracies": false, "accuracy_explanation": "", "completeness_score": 4, "relevance_score": 5, "clarity_score": 5}'}, 'user_utility_rating': None, 'stage3_time': 1.3113021850585938e-05}
2025-04-02 14:27:06,940 - INFO - app.py:33 - --- Starting Streamlit App ---
2025-04-02 14:27:06,940 - INFO - app.py:40 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-02 14:27:06,943 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743586016.697772
2025-04-02 14:27:06,976 - DEBUG - app.py:604 - Displaying details for Log ID 2
2025-04-02 14:27:06,977 - DEBUG - app.py:65 - Displaying evaluation results for timestamp: 1743585455.3297741
2025-04-02 14:27:06,979 - INFO - app.py:631 - --- Streamlit App Re-Render Complete ---
2025-04-03 13:35:52,689 - INFO - app.py:26 - --- Starting Streamlit App ---
2025-04-03 13:35:56,019 - INFO - app.py:32 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-03 13:35:56,028 - DEBUG - app.py:238 - Initialized evaluation_log in session state.
2025-04-03 13:35:56,028 - DEBUG - app.py:241 - Initialized current_run_index in session state.
2025-04-03 13:35:56,029 - INFO - app.py:559 - --- Streamlit App Re-Render Complete ---
2025-04-03 13:36:01,626 - INFO - app.py:26 - --- Starting Streamlit App ---
2025-04-03 13:36:04,915 - INFO - app.py:32 - Successfully imported config, llm_interface, and evaluation_metrics.
2025-04-03 13:36:04,927 - DEBUG - app.py:238 - Initialized evaluation_log in session state.
2025-04-03 13:36:04,928 - DEBUG - app.py:241 - Initialized current_run_index in session state.
2025-04-03 13:36:04,955 - INFO - app.py:559 - --- Streamlit App Re-Render Complete ---
